\section{Evaluation}
\label{sec:evaluation}

As discusssed in Section~\ref{subsubsec:efficiencyanddeterminism} our approach 
provides worst-case time bounds to all monitoring operations making it 
time-deterministic. Moreover, the approach is complete and guarantees no false 
positives. However, it was was important to understand its effectiveness in the 
memory reduction and the error reporting. Hence, we focused on two research 
questions.

{\bf RQ1:} Does our approach lead to substantially save the memory consumed by 
runtime monitors?

{\bf RQ2:} Can our approach effectively catch property violations?

\subsection{Implemetation and Artifacts}
\label{subsec:implementation}

Our implementation is semi-automatic and is based on the refinement of the 
aspects generated by JavaMOP 2.3. The reason for choosing an older version of 
JavaMOP was that it is simple to understand and it performs less optimizations. 
Hence, the chance of interfering the existing optimizations with our approach is 
less. This would not have been possible if we did not have a good understanding 
of the aspects consumed by our prototype implementation. Moreover, we compare 
the results of our work with JavaMOP 2.3 as well as JavaMOP 3.0 which is a much 
newer version of JavaMOP. The aspects are then woven using ajc 1.7 compiler, and 
the resulting Java program is executed on HPC server running Cent OS 6.5 and JVM 
1.7.0. 

We referred to previous studies \cite{}, and identified a few challenging DaCapo 
benchmarks and Java standard library property combinations that have been found 
difficult to b consuming a large amount of memory. Hence, we chose three DaCapo 
benchmarks \textsf{avrora}, \textsf{bloat}, and \textsf{pmd}. The benchmark 
\textsf{avrora} was taken from DaCapo 2010 benchmark suite since it was not 
apart of the 2006 benchmark suite, whereas the other two were taken from the 
2006 benchmark suite since \textsf{bloat} was not a part of the 2010 benchmark 
suite. We chose two properties \texttt{HasNext} and \texttt{UnsafeIterator} 
described in Section~\ref{sec:motivation} since they were found to be generating 
a large number of monitors. Property \texttt{HasNext} was particularly useful 
since two of the three benchmarks reported property violations which we could 
use for the comparison in our study. For this property, we added an extra 
\textit{creation} event for the convenience so that the monitor creation happens 
only  for the first time the system encounters the object. Property 
\texttt{UnsafeIterator} already had a well-defined creation event.


\myparagraph{DaCapo instrumentation} We have used \texttt{Soot}~\cite{soot} to
instrument Dacapo benchmarks. \code{getStackTrace()} method can be used to
fetch current stack trace but it introduces overhead. To mitigate this, we have
instrumented each of the methods of DaCapo benchmark with a static
\code{integer} field which is populated with an unique method id represented by
a $16$ bit \code{integer}. We have also simulate the stack trace by using a
circular array which contains these method id's. The circular array is
implemented by a $64$ bit \code{long}. A pseudo code of the method stack is
described in Code~\ref{snippet:methodTrace}.

\lstset{escapeinside={/*@}{@*/}, language=Java , caption=\bf Method trace
implementation., label=snippet:methodTrace} \begin{figure}[t]
\begin{lstlisting}
long trace;
int counter = 0;
void methodTrace(int methodID) {
 switch (counter) {
  case 0: id <<= 32; break;
  case 1: id <<= 16; break
  case 2: id <<= 0; break;
  default: break;
 }
 trace |= id;
 trace &= 0xffffffffffffL;
 counter = (counter + 1) % traceLength;
}
\end{lstlisting}
\end{figure}

The time and memory overheads have been reported after using -converge option 
provide by the DaCapo benchmark suite.

\subsection{Results}
\label{sec:results}

\paragraph{RQ1: Memory Saving}

Table~\ref{table:memoryusage} summarizes the memory usage of the benchmarks when 
monitored for the two properties.



\ignore{
Table  summarizes the typestate violations reported and the monitors generated 
while monitoring for the HasNext property. In this case, if a match is found for 
the method call trace, the monitoring may or may not be done depending on the 
frequency of that object being monitored. It shows that the number of monitors 
generated at runtime are far less than the monitors of original JavaMOP Aspect. 
For avrora, there is no property violations reported for the original JavaMOP 
aspect and thus we consider bloat and pmd only for violations reported. In 
bloat, we have been able to capture 69\% of violations by generating only 2.88\% 
of monitor instances while in pmd for 0.51\% of monitors, 40\% of violations are 
captured.}


\begin{table*}[!ht]
\centering
\begin{tabular}{|p{4cm}|c|c|c|c|c|c|}
\hline
  \multirow{2}{*}{}                                 & 
\multicolumn{3}{c|}{HasNext}           & \multicolumn{3}{c|}{FailSafeIter}       
      \\ \cline{2-7}                                              
                    & avrora           & bloat         & pmd      & avrora       
    & bloat         & pmd \\ \hline
Unoptimized JavaMOP Aspect     & 1.09 & 0.50 & 0.70  & 1.29 & 0.69 & 0.99 \\ 
\hline
Optimised JavaMOP Aspect (limit=100)   & 0.5  & 0.5  & 0.5   & 0.5  & 0.5  & 0.5 
  \\ \hline
Optimised JavaMOP Aspect (limit=1000)   & 0.5  & 0.5  & 0.5   & 0.5  & 0.5  & 
0.5   \\ \hline
Optimised JavaMOP Aspect (limit=10000)   & 0.5  & 0.5  & 0.5   & 0.5  & 0.5  & 
0.5   \\ \hline

\end{tabular}
\caption{Peak memory usage by the DaCapo benchmarks (in gb). Percentage overhead 
in the parantheses.}
\end{table*}
\label{table:memoryusage}


\paragraph{RQ2: Error Reporting}

In order to understand the effectiveness of our approach, we monitored the 
benchmarks for \texttt{HasNext} property since the property 
texttt{UnsafeIterator} does not produce
any errors. We varied the size of the global monitor pool and observed the 
errors that were reported. Table~\ref{table:errorreporting1} presents our 
findings.



\begin{table*}[!ht]
\centering
\begin{tabular}{|p{3.7cm}|p{1.2cm}|p{1.5cm}|p{1.2cm}|p{1.5cm}|p{1.2cm}|p{1.5cm}|
}
\hline
\multirow{2}{*}{}               & \multicolumn{2}{c|}{avrora}             & 
\multicolumn{2}{c|}{bloat}            & \multicolumn{2}{c|}{pmd}              \\ 
\cline{2-7} 
                                       & \# Errors Reported  & \# Monitor 
Allocations & \# Errors reported & \# Monitor Allocations& \# Errors Reported & 
\# Monitor Allocations\\ \hline
Original JavaMOP Aspect                                                    & 0   
           & 0.91M         & 42          & 1.89M            & 333               
& 1.95M     \\ \hline
Optimized JavaMOP Aspect  \# Monitors Limit= 100            & 0             & 
1.45k           & 0            & 54.6k              & 6                  & 
10.03k       \\ \hline
Optimized JavaMOP Aspect  \# Monitors Limit=1k           & 0             & 1.45k 
          & 0            & 54.6k             & 34.6              & 10.03k       
\\ \hline
Optimized JavaMOP Aspect  \# Monitors Limit=10k        & 0             & 1.45k   
        & 0            & 54.6k             & 130               & 10.03k       \\ 
\hline
Optimized JavaMOP Aspect  \# Monitors Limit=100k      & 0             & 1.45k    
       & 29          & 54.6k             & 131               & 10.03k       \\ 
\hline
\end{tabular}
\caption{Errors reported and monitors generated for \texttt{HasNext} Property.}
\end{table*}
\label{table:errorreporting1}

All monitors that were allocated by the original JavaMO were physically distinct 
objects. In other words, the number of allocations matched number of creations. 
However, in our approach the number of allocated monitors was often smaller than 
the number of created monitors owing to the reuse of monitors. We observe that 
the number of monitor allocations, in general, has reduced drastically by up to 
three orders of magnitude when compared with the monitors that were created by 
the original JavaMOP monitoring system. We also observe that only \textit{bloat} 
and \textit{pmd} report errors. The number of errors reported is based on the 
average number of reports taken over 20 runs. The number increases as the size 
of the monitor pool increase from 100 to 100k. For 100k monitor pool size, our 
technique \textit{bloat} catches 29 violations (69\%), where as for \textit{pmd} 
it catches 131 violations (39\%). The technique achieves this by allocating 
54.6k monitors (3\%) for \textit{bloat} and 10.03k monitors for \
textit{pmd}.

We manually inspected the error reports generated by the original JavaMOP 
monitors and we realized that out of the total 42 violations reported for 
\textit{bloat}, only two were distinct violations pertaining to their execution 
context. The rest of the error reports were identical to one of two distinct 
errors. In short, we could partition all 42 error reports into two equivalence 
classes. Similarly, we could divide all 333 error reports for \textit{pmd} into 
3 equivalence classes. When we inspected the errors reported by our technique, 
both of these errors were reported by all settings that reported violations. In 
other words, the monitors showed considerable redundancy in their behavior, and 
our heuristic to use programming context as a means to control the monitor 
allocation process was well-founded.

In order to understand the effect of execution context on the error reported, we 
conducted another study in which we did not allocate a monitor if the context 
match was found. In other words we created exactly one monitor corresponding 
every distinct context. Table~\ref{table:errorreporting2} summarizes the data in 
which we see that we could catch all of the distinct violations for 
\textit{bloat} and \textit{pmd} by generating only 1.1k (0.06\%) and 137 
(0.007\%) monitors respectively corresponding to the execution contexts. This is 
a huge reduction in the number of monitors, and it indicates that program 
execution context can be a very effective heuristic in the process of 
controlling monitor allocations.

These results indicate that the approach can effectively report errors.


\begin{table*}[ht]
\centering
\begin{tabular}{|p{3.7cm}|p{1.2cm}|p{1.5cm}|p{1.2cm}|p{1.5cm}|p{1.2cm}|p{1.5cm}|
}
\hline
\multirow{2}{*}{}               & \multicolumn{2}{c|}{avrora}             & 
\multicolumn{2}{c|}{bloat}            & \multicolumn{2}{c|}{pmd}              \\ 
\cline{2-7} 
                                       & \# Errors Reported  & \# Monitor 
Allocations & \# Errors reported & \# Monitor Allocations& \# Errors Reported & 
\# Monitor Allocations\\ \hline
Optimized JavaMOP Aspect \# Monitors Limit= 100            & 0             & 66  
          & 2           & 1.1k              & 3               & 137     \\ 
\hline
Optimized JavaMOP Aspect \# Monitors Limit=1k           & 0             & 66     
      & 2            & 1.1k              & 3               & 137       \\ \hline
%Optimized JavaMOP Aspect \# Monitors Limit=10k        & 0             & 66      
     & 2            & 1.1k              & 3               &137       \\ \hline
%Optimized JavaMOP Aspect \# Monitors Limit=100k      & 0             & 66       
    & 2            & 1.1k              & 3               & 137       \\ \hline
\end{tabular}
\caption{Errors reported and monitors generated for \texttt{HasNext} Property 
when monitors were not allocated after a match was found.}
\end{table*}
\label{table:errorreporting2}


\subsection{Threats to Validity}
\label{subsec:threats}

Our study is restricted to three DaCapo benchmarks and two Java standard library 
properties. Even though these combinations have been found to be challenging to 
monitor in the past, it is possible that the combinations are not representative 
and results of the study will change if we include more combinations. We intend 
to this extended study in the future.

We used JavaMOP 2.3 as a baseline tool for our prototype implementation. 
However, the results of the study may change if we use a different tool or a 
more recent version of JavaMOP. However, using an older version of JavaMOP had 
an advantage of being simpler and easier to understand which allowed us to 
ensure that our optimizations do not interfere with JavaMOP's optimizations. 
Moreover, our goal is not compare the performance with JavaMOP, but only to show 
that our technique is complementary to JavaMOP optimizations and can be used to 
extend JavaMOP which would add further to its effectiveness.

The choices of hardware and software platforms, in particular, the server 
settings may influence the results. In the future we plan to repeat the study on 
a variety of platforms to understand their impact on the results.


\subsection{Strengths and Weaknesses}
\label{subsec:weaknesses}

The results presented in Section~\ref{subsec:results} and the discussion in 
Section~\ref{subsubsec:efficiencyanddeterminism} indicate that the approach can 
i) effectively control the memory needs, ii) does not compromise much with the 
error reporting, iii) provides worst-case bounds to all monitoring operations, 
and iv) does not produce false positives which preserves its completeness. 
However, current implementation does not efficiently control the runtime 
overhead. Table~\ref{table:runtimeoverhead} provides the runtime overheads for 
the same DaCapo benchmarks and program properties used previously in this 
section. The monitoring performed by our implementation is much slower than that 
performed by the original JavaMOP. This slowdown is caused due to the expensive 
Java operation to get the current thread and its stack trace 
(Thread.currentThread().getStackTrace()). This penalty is particularly high in 
the server environment in which we conducted our study. In the future, we plan 
to develop a hybrid approach 
that combines our context-aware and randomized approaches to reduce this 
overhead. We also plan to conduct a study to understand how the stack trace 
overhead differs on various hardware and software systems. Another approach that 
we are considering is to employ a static analysis to analyze the call graph of a 
program and then instrument the program that simulates the stack trace by 
traversing the context tree that is built from the call graph. This approach 
would be light-weight compared to the current mechanism that depends on system 
services and hence, it would help reduce the overhead. We leave this as the 
future work.


\begin{table*}[t]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{}                                                                
     & \multicolumn{3}{c|}{HasNext Property} & 
\multicolumn{3}{c|}{UnsafeIterator Property} \\ \cline{2-7} 
                                                                                 
     & avrora       & bloat       & pmd      & avrora         & bloat         & 
pmd         \\ \hline
\begin{tabular}[c]{@{}c@{}}Original JavaMOP Aspect \end{tabular} &  6.7          
 &7.53             & 6.5        & 15.2          & 10.8         & 9.8        \\ 
\hline
\begin{tabular}[c]{@{}c@{}}Optimized JavaMOP  \# Monitors Limit=100\end{tabular} 
       & 33.9             &64.1             & 64.5          &  19.6              
&56.8              &62.5         \\ \hline
\begin{tabular}[c]{@{}c@{}}Optimized JavaMop \# Monitors Limit=1k)\end{tabular}  
     & 31.0             & 59.7            & 60.6          & 21.0        & 57.1   
           & 59.9     \\ \hline
\begin{tabular}[c]{@{}c@{}}Optimized JavaMop\# Monitors Limit=10k\end{tabular}   
   & 39.0             & 61.0            & 61.0         & 19.4            & 57.3  
            & 60.0  \\ \hline
\end{tabular}
\caption{Comparison of runtime overhead (time in seconds).}
\end{table*}
\label{table:runtimeoverhead}


%\clearpage
\ignore{
Table 5.3 reports the runtime overhead of the typestate monitoring when applied 
to the benchmarks for HasNext and UnsafeIterator property. We observe a less 
promising performance in terms of runtime. The monitoring performed by our 
algorithm is much slower than performed by the original JavaMOP aspect. This 
slowdown is caused due to the expensive Java operation to get the current thread 
and its stack trace (Thread.currentThread().getStackTrace()). This penalty will 
be a lot heavier in server environment. However, on skipping this operation and 
perform monitoring randomly, we observed a huge drop in overhead - runtime was 
even less than the runtime of original JavaMOP aspect. But we may miss some 
violations by this heuristic which is bigger trade off. Missing the violations 
can never be a good idea so, we choose to exhibit the runtime overhead but 
memory usage will be very less.
}


